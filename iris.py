# -*- coding: utf-8 -*-
"""Iris.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16QI14F46A5taOo4_vgGbIv0S-eMzNtam
"""

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.linear_model import LogisticRegression

# -----------------------------
# 1. Load and prepare dataset
# -----------------------------
iris = load_iris()
X = iris.data[:, [0, 2]]  # sepal length, petal length
y = iris.target

# Select only two classes: setosa (0) and versicolor (1)
mask = y < 2
X = X[mask]
y = y[mask]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# -----------------------------
# 2. Core Logistic Regression functions
# -----------------------------
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def binary_cross_entropy(y_true, y_pred):
    eps = 1e-9
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

def compute_gradients(X, y, y_pred):
    m = X.shape[0]
    dw = (1 / m) * np.dot(X.T, (y_pred - y))
    db = (1 / m) * np.sum(y_pred - y)
    return dw, db

# -----------------------------
# 3. Training loop (Gradient Descent)
# -----------------------------
def train_logistic_regression(X, y, lr=0.1, n_iters=1000):
    n_features = X.shape[1]
    w = np.zeros(n_features)
    b = 0

    for _ in range(n_iters):
        linear_output = np.dot(X, w) + b
        y_pred = sigmoid(linear_output)

        dw, db = compute_gradients(X, y, y_pred)

        w -= lr * dw
        b -= lr * db

    return w, b

# Train model
weights, bias = train_logistic_regression(X_train, y_train)

# -----------------------------
# 4. Predictions and evaluation
# -----------------------------
def predict(X, w, b, threshold=0.5):
    probs = sigmoid(np.dot(X, w) + b)
    return (probs >= threshold).astype(int)

y_pred_custom = predict(X_test, weights, bias)

custom_accuracy = accuracy_score(y_test, y_pred_custom)
custom_precision = precision_score(y_test, y_pred_custom)
custom_recall = recall_score(y_test, y_pred_custom)

# -----------------------------
# 5. Scikit-learn comparison
# -----------------------------
sk_model = LogisticRegression()
sk_model.fit(X_train, y_train)

y_pred_sk = sk_model.predict(X_test)

sk_accuracy = accuracy_score(y_test, y_pred_sk)
sk_precision = precision_score(y_test, y_pred_sk)
sk_recall = recall_score(y_test, y_pred_sk)

# -----------------------------
# 6. Results
# -----------------------------
print("Custom Logistic Regression")
print("Accuracy:", custom_accuracy)
print("Precision:", custom_precision)
print("Recall:", custom_recall)

print("\nScikit-learn Logistic Regression")
print("Accuracy:", sk_accuracy)
print("Precision:", sk_precision)
print("Recall:", sk_recall)

print("\nLearned Weights:", weights)
print("Learned Bias:", bias)